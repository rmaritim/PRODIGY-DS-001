{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\DELL\\\\OneDrive\\\\Capstone_Project\\\\data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m data_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDELL\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mCapstone_Project\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Combine the datasets\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m combined_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcombine_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Display info about the combined dataset\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(combined_dataset\u001b[38;5;241m.\u001b[39minfo())\n",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m, in \u001b[0;36mcombine_datasets\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m      8\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, filename)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Read the CSV file\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDELL\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mOneDrive\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mCapstone_Project\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Add a column to identify the source file (optional)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource_File\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m filename\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive\\Capstone_Project\\virt_caps\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive\\Capstone_Project\\virt_caps\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive\\Capstone_Project\\virt_caps\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive\\Capstone_Project\\virt_caps\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive\\Capstone_Project\\virt_caps\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\DELL\\\\OneDrive\\\\Capstone_Project\\\\data'"
     ]
    }
   ],
   "source": [
    "# def combine_datasets(directory):\n",
    "#     # List to hold all dataframes\n",
    "#     all_dataframes = []\n",
    "    \n",
    "#     # Iterate through all CSV files in the directory\n",
    "#     for filename in os.listdir(directory):\n",
    "#         if filename.endswith('.csv'):\n",
    "#             file_path = os.path.join(directory, filename)\n",
    "            \n",
    "#             # Read the CSV file\n",
    "#             df = pd.read_csv(\"C:\\\\Users\\\\DELL\\\\OneDrive\\\\Capstone_Project\\\\data\")\n",
    "            \n",
    "#             # Add a column to identify the source file (optional)\n",
    "#             df['Source_File'] = filename\n",
    "            \n",
    "#             # Append to the list\n",
    "#             all_dataframes.append(df)\n",
    "    \n",
    "#     # Concatenate all dataframes vertically\n",
    "#     combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "#     return combined_df\n",
    "\n",
    "# # Specify the directory containing your CSV files\n",
    "# data_directory = \"C:\\\\Users\\\\DELL\\\\OneDrive\\\\Capstone_Project\\\\data\"\n",
    "\n",
    "# # Combine the datasets\n",
    "# combined_dataset = combine_datasets(data_directory)\n",
    "\n",
    "# # Display info about the combined dataset\n",
    "# print(combined_dataset.info())\n",
    "\n",
    "# # Save the combined dataset\n",
    "# combined_dataset.to_csv('combined_sales_data_2019.csv', index=False)\n",
    "\n",
    "# print(\"Combined dataset has been saved as 'combined_sales_data_2019.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 186850 entries, 0 to 186849\n",
      "Data columns (total 11 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   Order ID          85380 non-null   object \n",
      " 1   Product           186305 non-null  object \n",
      " 2   Quantity Ordered  85380 non-null   object \n",
      " 3   Price Each        85380 non-null   object \n",
      " 4   Order Date        85380 non-null   object \n",
      " 5   Purchase Address  85380 non-null   object \n",
      " 6   Order_ID          100730 non-null  float64\n",
      " 7   Quantity_Ordered  100730 non-null  float64\n",
      " 8   Price_Each        100730 non-null  float64\n",
      " 9   Order_Date        100730 non-null  object \n",
      " 10  Purchase_Address  100925 non-null  object \n",
      "dtypes: float64(3), object(8)\n",
      "memory usage: 15.7+ MB\n",
      "None\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(combined_dataset\u001b[38;5;241m.\u001b[39minfo())\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Optionally, save the combined dataset to a new CSV file\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mcombined_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive\\Capstone_Project\\virt_caps\\lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive\\Capstone_Project\\virt_caps\\lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive\\Capstone_Project\\virt_caps\\lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive\\Capstone_Project\\virt_caps\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive\\Capstone_Project\\virt_caps\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'data'"
     ]
    }
   ],
   "source": [
    "# # import pandas as pd\n",
    "# # import os\n",
    "\n",
    "# # Directory containing the datasets\n",
    "# data_directory = \"data\"\n",
    "\n",
    "# # Initialize an empty list to store the DataFrames\n",
    "# dfs = []\n",
    "\n",
    "# # Loop through all files in the directory\n",
    "# for filename in os.listdir(data_directory):\n",
    "#     if filename.endswith(\".csv\"):  # Check if the file is a CSV file\n",
    "#         file_path = os.path.join(data_directory, filename)\n",
    "        \n",
    "#         # Load the CSV file into a DataFrame\n",
    "#         df = pd.read_csv(file_path)\n",
    "        \n",
    "#         # Append the DataFrame to the list\n",
    "#         dfs.append(df)\n",
    "\n",
    "# # Concatenate all DataFrames in the list into one DataFrame\n",
    "# combined_dataset = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# # Display info about the combined dataset\n",
    "# print(combined_dataset.info())\n",
    "\n",
    "# # Optionally, save the combined dataset to a new CSV file\n",
    "# combined_dataset.to_csv(\"data\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 186850 entries, 0 to 186849\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   Order ID          85380 non-null   object \n",
      " 1   Product           186305 non-null  object \n",
      " 2   Quantity Ordered  85380 non-null   object \n",
      " 3   Price Each        85380 non-null   object \n",
      " 4   Order Date        85380 non-null   object \n",
      " 5   Purchase Address  85380 non-null   object \n",
      " 6   Source_File       186850 non-null  object \n",
      " 7   Order_ID          100730 non-null  float64\n",
      " 8   Quantity_Ordered  100730 non-null  float64\n",
      " 9   Price_Each        100730 non-null  float64\n",
      " 10  Order_Date        100730 non-null  object \n",
      " 11  Purchase_Address  100925 non-null  object \n",
      "dtypes: float64(3), object(9)\n",
      "memory usage: 17.1+ MB\n",
      "None\n",
      "Combined dataset has been saved as 'C:\\Users\\DELL\\OneDrive\\Capstone_Project\\data\\combined_sales_data_2019.csv'\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# \n",
    "\n",
    "def combine_csv_files(directory):\n",
    "    # Use glob to get all CSV files in the directory\n",
    "    all_files = glob.glob(os.path.join(directory, \"*.csv\"))\n",
    "    \n",
    "    # List to store individual dataframes\n",
    "    df_list = []\n",
    "    \n",
    "    for filename in all_files:\n",
    "        try:\n",
    "            df = pd.read_csv(filename, encoding='utf-8')\n",
    "            df['Source_File'] = os.path.basename(filename)  # Add filename as a column\n",
    "            df_list.append(df)\n",
    "        except PermissionError:\n",
    "            print(f\"Permission denied for file: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {filename}: {str(e)}\")\n",
    "    \n",
    "    # Concatenate all dataframes in the list\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Specify the directory containing your CSV files\n",
    "data_directory = r\"C:\\Users\\DELL\\OneDrive\\Capstone_Project\\data\"\n",
    "\n",
    "# Combine the datasets\n",
    "try:\n",
    "    combined_dataset = combine_csv_files(data_directory)\n",
    "    \n",
    "    # Display info about the combined dataset\n",
    "    print(combined_dataset.info())\n",
    "    \n",
    "    # Save the combined dataset\n",
    "    output_file = os.path.join(data_directory, \"combined_sales_data_2019.csv\")\n",
    "    combined_dataset.to_csv(output_file, index=False)\n",
    "    print(f\"Combined dataset has been saved as '{output_file}'\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Order ID                     Product Quantity Ordered Price Each  \\\n",
      "0   176558        USB-C Charging Cable                2      11.95   \n",
      "1      NaN                         NaN              NaN        NaN   \n",
      "2   176559  Bose SoundSport Headphones                1      99.99   \n",
      "3   176560                Google Phone                1        600   \n",
      "4   176560            Wired Headphones                1      11.99   \n",
      "\n",
      "       Order Date                      Purchase Address           Source_File  \\\n",
      "0  04/19/19 08:46          917 1st St, Dallas, TX 75001  Sales_April_2019.csv   \n",
      "1             NaN                                   NaN  Sales_April_2019.csv   \n",
      "2  04/07/19 22:30     682 Chestnut St, Boston, MA 02215  Sales_April_2019.csv   \n",
      "3  04/12/19 14:38  669 Spruce St, Los Angeles, CA 90001  Sales_April_2019.csv   \n",
      "4  04/12/19 14:38  669 Spruce St, Los Angeles, CA 90001  Sales_April_2019.csv   \n",
      "\n",
      "   Order_ID  Quantity_Ordered  Price_Each Order_Date Purchase_Address  \n",
      "0       NaN               NaN         NaN        NaN              NaN  \n",
      "1       NaN               NaN         NaN        NaN              NaN  \n",
      "2       NaN               NaN         NaN        NaN              NaN  \n",
      "3       NaN               NaN         NaN        NaN              NaN  \n",
      "4       NaN               NaN         NaN        NaN              NaN  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Replace 'your_file.csv' with the path to your CSV file\n",
    "combined_data = pd.read_csv('data/combined_sales_data_2019.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(combined_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity Ordered</th>\n",
       "      <th>Price Each</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Purchase Address</th>\n",
       "      <th>Source_File</th>\n",
       "      <th>Order_ID</th>\n",
       "      <th>Quantity_Ordered</th>\n",
       "      <th>Price_Each</th>\n",
       "      <th>Order_Date</th>\n",
       "      <th>Purchase_Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>186845</th>\n",
       "      <td>NaN</td>\n",
       "      <td>AAA Batteries (4-pack)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sales_September_2019.csv</td>\n",
       "      <td>259353.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.99000</td>\n",
       "      <td>2017-09-19 20:56:00.0000000</td>\n",
       "      <td>840 Highland St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186846</th>\n",
       "      <td>NaN</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sales_September_2019.csv</td>\n",
       "      <td>259354.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>700.00000</td>\n",
       "      <td>2001-09-19 16:00:00.0000000</td>\n",
       "      <td>216 Dogwood St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186847</th>\n",
       "      <td>NaN</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sales_September_2019.csv</td>\n",
       "      <td>259355.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>700.00000</td>\n",
       "      <td>2023-09-19 07:39:00.0000000</td>\n",
       "      <td>220 12th St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186848</th>\n",
       "      <td>NaN</td>\n",
       "      <td>34in Ultrawide Monitor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sales_September_2019.csv</td>\n",
       "      <td>259356.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>379.98999</td>\n",
       "      <td>2019-09-19 17:30:00.0000000</td>\n",
       "      <td>511 Forest St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186849</th>\n",
       "      <td>NaN</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sales_September_2019.csv</td>\n",
       "      <td>259357.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.95000</td>\n",
       "      <td>2030-09-19 00:18:00.0000000</td>\n",
       "      <td>250 Meadow St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Order ID                 Product Quantity Ordered Price Each  \\\n",
       "186845      NaN  AAA Batteries (4-pack)              NaN        NaN   \n",
       "186846      NaN                  iPhone              NaN        NaN   \n",
       "186847      NaN                  iPhone              NaN        NaN   \n",
       "186848      NaN  34in Ultrawide Monitor              NaN        NaN   \n",
       "186849      NaN    USB-C Charging Cable              NaN        NaN   \n",
       "\n",
       "       Order Date Purchase Address               Source_File  Order_ID  \\\n",
       "186845        NaN              NaN  Sales_September_2019.csv  259353.0   \n",
       "186846        NaN              NaN  Sales_September_2019.csv  259354.0   \n",
       "186847        NaN              NaN  Sales_September_2019.csv  259355.0   \n",
       "186848        NaN              NaN  Sales_September_2019.csv  259356.0   \n",
       "186849        NaN              NaN  Sales_September_2019.csv  259357.0   \n",
       "\n",
       "        Quantity_Ordered  Price_Each                   Order_Date  \\\n",
       "186845               3.0     2.99000  2017-09-19 20:56:00.0000000   \n",
       "186846               1.0   700.00000  2001-09-19 16:00:00.0000000   \n",
       "186847               1.0   700.00000  2023-09-19 07:39:00.0000000   \n",
       "186848               1.0   379.98999  2019-09-19 17:30:00.0000000   \n",
       "186849               1.0    11.95000  2030-09-19 00:18:00.0000000   \n",
       "\n",
       "                               Purchase_Address  \n",
       "186845   840 Highland St, Los Angeles, CA 90001  \n",
       "186846  216 Dogwood St, San Francisco, CA 94016  \n",
       "186847     220 12th St, San Francisco, CA 94016  \n",
       "186848   511 Forest St, San Francisco, CA 94016  \n",
       "186849   250 Meadow St, San Francisco, CA 94016  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current DataFrame structure:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 186850 entries, 0 to 186849\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   Order ID          85380 non-null   object \n",
      " 1   Product           186305 non-null  object \n",
      " 2   Quantity Ordered  85380 non-null   object \n",
      " 3   Price Each        85380 non-null   object \n",
      " 4   Order Date        85380 non-null   object \n",
      " 5   Purchase Address  85380 non-null   object \n",
      " 6   Source_File       186850 non-null  object \n",
      " 7   Order_ID          100730 non-null  float64\n",
      " 8   Quantity_Ordered  100730 non-null  float64\n",
      " 9   Price_Each        100730 non-null  float64\n",
      " 10  Order_Date        100730 non-null  object \n",
      " 11  Purchase_Address  100925 non-null  object \n",
      "dtypes: float64(3), object(9)\n",
      "memory usage: 17.1+ MB\n",
      "None\n",
      "\n",
      "No duplicate columns detected. The DataFrame structure seems correct.\n",
      "\n",
      "Final DataFrame preview:\n",
      "  Order ID                     Product Quantity Ordered Price Each  \\\n",
      "0   176558        USB-C Charging Cable                2      11.95   \n",
      "1      NaN                         NaN              NaN        NaN   \n",
      "2   176559  Bose SoundSport Headphones                1      99.99   \n",
      "3   176560                Google Phone                1        600   \n",
      "4   176560            Wired Headphones                1      11.99   \n",
      "\n",
      "       Order Date                      Purchase Address           Source_File  \\\n",
      "0  04/19/19 08:46          917 1st St, Dallas, TX 75001  Sales_April_2019.csv   \n",
      "1             NaN                                   NaN  Sales_April_2019.csv   \n",
      "2  04/07/19 22:30     682 Chestnut St, Boston, MA 02215  Sales_April_2019.csv   \n",
      "3  04/12/19 14:38  669 Spruce St, Los Angeles, CA 90001  Sales_April_2019.csv   \n",
      "4  04/12/19 14:38  669 Spruce St, Los Angeles, CA 90001  Sales_April_2019.csv   \n",
      "\n",
      "   Order_ID  Quantity_Ordered  Price_Each Order_Date Purchase_Address  \n",
      "0       NaN               NaN         NaN        NaN              NaN  \n",
      "1       NaN               NaN         NaN        NaN              NaN  \n",
      "2       NaN               NaN         NaN        NaN              NaN  \n",
      "3       NaN               NaN         NaN        NaN              NaN  \n",
      "4       NaN               NaN         NaN        NaN              NaN  \n"
     ]
    }
   ],
   "source": [
    "# # import pandas as pd\n",
    "# # import os\n",
    "# # import glob\n",
    "\n",
    "# def analyze_and_fix_dataframe(file_path):\n",
    "#     # Read the current combined CSV file\n",
    "#     df = pd.read_csv(file_path)\n",
    "    \n",
    "#     print(\"Current DataFrame structure:\")\n",
    "#     print(df.info())\n",
    "    \n",
    "#     # Check if we have duplicate column names\n",
    "#     if len(df.columns) != len(set(df.columns)):\n",
    "#         print(\"\\nDetected duplicate column names. Fixing the issue...\")\n",
    "        \n",
    "#         # Get the original column names (assuming they're in the first 6 columns)\n",
    "#         original_columns = df.columns[:6].tolist()\n",
    "        \n",
    "#         # Reshape the DataFrame\n",
    "#         df_fixed = pd.DataFrame()\n",
    "#         for i in range(0, len(df.columns), 6):\n",
    "#             temp_df = df.iloc[:, i:i+6]\n",
    "#             temp_df.columns = original_columns\n",
    "#             df_fixed = pd.concat([df_fixed, temp_df], ignore_index=True)\n",
    "        \n",
    "#         print(\"\\nFixed DataFrame structure:\")\n",
    "#         print(df_fixed.info())\n",
    "        \n",
    "#         # Save the fixed DataFrame\n",
    "#         fixed_file_path = file_path.replace('.csv', '_fixed.csv')\n",
    "#         df_fixed.to_csv(fixed_file_path, index=False)\n",
    "#         print(f\"\\nFixed DataFrame saved to: {fixed_file_path}\")\n",
    "        \n",
    "#         return df_fixed\n",
    "#     else:\n",
    "#         print(\"\\nNo duplicate columns detected. The DataFrame structure seems correct.\")\n",
    "#         return df\n",
    "\n",
    "# # Specify the path to your combined CSV file\n",
    "# file_path = r\"C:\\Users\\DELL\\OneDrive\\Capstone_Project\\data\\combined_sales_data_2019.csv\"\n",
    "\n",
    "# # Analyze and fix the DataFrame\n",
    "# df_final = analyze_and_fix_dataframe(file_path)\n",
    "\n",
    "# # Now you can work with df_final, which should have the correct structure\n",
    "# print(\"\\nFinal DataFrame preview:\")\n",
    "# print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame structure:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 186850 entries, 0 to 186849\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   Order ID          85380 non-null   object \n",
      " 1   Product           186305 non-null  object \n",
      " 2   Quantity Ordered  85380 non-null   object \n",
      " 3   Price Each        85380 non-null   object \n",
      " 4   Order Date        85380 non-null   object \n",
      " 5   Purchase Address  85380 non-null   object \n",
      " 6   Source_File       186850 non-null  object \n",
      " 7   Order_ID          100730 non-null  float64\n",
      " 8   Quantity_Ordered  100730 non-null  float64\n",
      " 9   Price_Each        100730 non-null  float64\n",
      " 10  Order_Date        100730 non-null  object \n",
      " 11  Purchase_Address  100925 non-null  object \n",
      "dtypes: float64(3), object(9)\n",
      "memory usage: 17.1+ MB\n",
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot assemble with duplicate keys",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDELL\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCapstone_Project\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcombined_sales_data_2019.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Harmonize and merge the DataFrame\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m df_final \u001b[38;5;241m=\u001b[39m \u001b[43mharmonize_and_merge_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Now you can work with df_final, which should have the correct structure and data types\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal DataFrame preview:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 34\u001b[0m, in \u001b[0;36mharmonize_and_merge_dataframe\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     32\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantity_Ordered\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m safe_numeric(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantity_Ordered\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     33\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice_Each\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m safe_numeric(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice_Each\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 34\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOrder_Date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOrder_Date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcoerce\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Drop rows with all NaN values\u001b[39;00m\n\u001b[0;32m     37\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdropna(how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive\\Capstone_Project\\virt_caps\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1070\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1068\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[38;5;241m.\u001b[39mMutableMapping)):\n\u001b[1;32m-> 1070\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_assemble_from_unit_mappings\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, Index):\n\u001b[0;32m   1072\u001b[0m     cache_array \u001b[38;5;241m=\u001b[39m _maybe_cache(arg, \u001b[38;5;28mformat\u001b[39m, cache, convert_listlike)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive\\Capstone_Project\\virt_caps\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1165\u001b[0m, in \u001b[0;36m_assemble_from_unit_mappings\u001b[1;34m(arg, errors, utc)\u001b[0m\n\u001b[0;32m   1163\u001b[0m arg \u001b[38;5;241m=\u001b[39m DataFrame(arg)\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[1;32m-> 1165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assemble with duplicate keys\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;66;03m# replace passed unit with _unit_map\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(value):\n",
      "\u001b[1;31mValueError\u001b[0m: cannot assemble with duplicate keys"
     ]
    }
   ],
   "source": [
    "# dd\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# def harmonize_and_merge_dataframe(file_path):\n",
    "#     # Read the CSV file\n",
    "#     df = pd.read_csv(file_path)\n",
    "    \n",
    "#     print(\"Original DataFrame structure:\")\n",
    "#     print(df.info())\n",
    "    \n",
    "#     # Define column mappings\n",
    "#     column_mappings = {\n",
    "#         'Order ID': 'Order_ID',\n",
    "#         'Quantity Ordered': 'Quantity_Ordered',\n",
    "#         'Price Each': 'Price_Each',\n",
    "#         'Order Date': 'Order_Date',\n",
    "#         'Purchase Address': 'Purchase_Address'\n",
    "#     }\n",
    "    \n",
    "#     # Rename columns\n",
    "#     df = df.rename(columns=column_mappings)\n",
    "    \n",
    "#     # Function to convert to numeric, handling errors\n",
    "#     def safe_numeric(x):\n",
    "#         try:\n",
    "#             return pd.to_numeric(x)\n",
    "#         except:\n",
    "#             return np.nan\n",
    "    \n",
    "#     # Correct data types\n",
    "#     df['Order_ID'] = safe_numeric(df['Order_ID'])\n",
    "#     df['Quantity_Ordered'] = safe_numeric(df['Quantity_Ordered'])\n",
    "#     df['Price_Each'] = safe_numeric(df['Price_Each'])\n",
    "#     df['Order_Date'] = pd.to_datetime(df['Order_Date'], errors='coerce')\n",
    "    \n",
    "#     # Drop rows with all NaN values\n",
    "#     df = df.dropna(how='all')\n",
    "    \n",
    "#     # Select and reorder columns\n",
    "#     columns_to_keep = ['Order_ID', 'Product', 'Quantity_Ordered', 'Price_Each', 'Order_Date', 'Purchase_Address', 'Source_File']\n",
    "#     df = df[columns_to_keep]\n",
    "    \n",
    "#     print(\"\\nHarmonized DataFrame structure:\")\n",
    "#     print(df.info())\n",
    "    \n",
    "#     # Save the harmonized DataFrame\n",
    "#     harmonized_file_path = file_path.replace('.csv', '_harmonized.csv')\n",
    "#     df.to_csv(harmonized_file_path, index=False)\n",
    "#     print(f\"\\nHarmonized DataFrame saved to: {harmonized_file_path}\")\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # Specify the path to your combined CSV file\n",
    "# file_path = r\"C:\\Users\\DELL\\OneDrive\\Capstone_Project\\data\\combined_sales_data_2019.csv\"\n",
    "\n",
    "# # Harmonize and merge the DataFrame\n",
    "# df_final = harmonize_and_merge_dataframe(file_path)\n",
    "\n",
    "# # Now you can work with df_final, which should have the correct structure and data types\n",
    "# print(\"\\nFinal DataFrame preview:\")\n",
    "# print(df_final.head())\n",
    "\n",
    "# # Basic data quality checks\n",
    "# print(\"\\nData quality checks:\")\n",
    "# print(f\"Number of null values:\\n{df_final.isnull().sum()}\")\n",
    "# print(f\"\\nNumber of unique values in each column:\\n{df_final.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame structure:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 186850 entries, 0 to 186849\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   Order ID          85380 non-null   object \n",
      " 1   Product           186305 non-null  object \n",
      " 2   Quantity Ordered  85380 non-null   object \n",
      " 3   Price Each        85380 non-null   object \n",
      " 4   Order Date        85380 non-null   object \n",
      " 5   Purchase Address  85380 non-null   object \n",
      " 6   Source_File       186850 non-null  object \n",
      " 7   Order_ID          100730 non-null  float64\n",
      " 8   Quantity_Ordered  100730 non-null  float64\n",
      " 9   Price_Each        100730 non-null  float64\n",
      " 10  Order_Date        100730 non-null  object \n",
      " 11  Purchase_Address  100925 non-null  object \n",
      "dtypes: float64(3), object(9)\n",
      "memory usage: 17.1+ MB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_25216\\1801676308.py:36: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Order_Date'] = pd.to_datetime(df['Order_Date'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Harmonized DataFrame structure:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 185950 entries, 0 to 373699\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count   Dtype         \n",
      "---  ------            --------------   -----         \n",
      " 0   Order_ID          185686 non-null  float64       \n",
      " 1   Product           185938 non-null  object        \n",
      " 2   Quantity_Ordered  185686 non-null  float64       \n",
      " 3   Price_Each        185686 non-null  float64       \n",
      " 4   Order_Date        185686 non-null  datetime64[ns]\n",
      " 5   Purchase_Address  185698 non-null  object        \n",
      " 6   Source_File       185950 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(3), object(3)\n",
      "memory usage: 11.3+ MB\n",
      "None\n",
      "\n",
      "Harmonized DataFrame saved to: C:\\Users\\DELL\\OneDrive\\Capstone_Project\\data\\combined_sales_data_2019_harmonized.csv\n",
      "\n",
      "Final DataFrame preview:\n",
      "   Order_ID                     Product  Quantity_Ordered  Price_Each  \\\n",
      "0  176558.0        USB-C Charging Cable               2.0       11.95   \n",
      "1       NaN                         NaN               NaN         NaN   \n",
      "2  176559.0  Bose SoundSport Headphones               1.0       99.99   \n",
      "3  176560.0                Google Phone               1.0      600.00   \n",
      "4  176560.0            Wired Headphones               1.0       11.99   \n",
      "\n",
      "           Order_Date                      Purchase_Address  \\\n",
      "0 2019-04-19 08:46:00          917 1st St, Dallas, TX 75001   \n",
      "1                 NaT                                   NaN   \n",
      "2 2019-04-07 22:30:00     682 Chestnut St, Boston, MA 02215   \n",
      "3 2019-04-12 14:38:00  669 Spruce St, Los Angeles, CA 90001   \n",
      "4 2019-04-12 14:38:00  669 Spruce St, Los Angeles, CA 90001   \n",
      "\n",
      "            Source_File  \n",
      "0  Sales_April_2019.csv  \n",
      "1  Sales_April_2019.csv  \n",
      "2  Sales_April_2019.csv  \n",
      "3  Sales_April_2019.csv  \n",
      "4  Sales_April_2019.csv  \n",
      "\n",
      "Data quality checks:\n",
      "Number of null values:\n",
      "Order_ID            264\n",
      "Product              12\n",
      "Quantity_Ordered    264\n",
      "Price_Each          264\n",
      "Order_Date          264\n",
      "Purchase_Address    252\n",
      "Source_File           0\n",
      "dtype: int64\n",
      "\n",
      "Number of unique values in each column:\n",
      "Order_ID            178437\n",
      "Product                 20\n",
      "Quantity_Ordered         9\n",
      "Price_Each              28\n",
      "Order_Date          142395\n",
      "Purchase_Address    140788\n",
      "Source_File             12\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def harmonize_and_merge_dataframe(file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    print(\"Original DataFrame structure:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    # Define column mappings\n",
    "    column_mappings = {\n",
    "        'Order ID': 'Order_ID',\n",
    "        'Quantity Ordered': 'Quantity_Ordered',\n",
    "        'Price Each': 'Price_Each',\n",
    "        'Order Date': 'Order_Date',\n",
    "        'Purchase Address': 'Purchase_Address'\n",
    "    }\n",
    "    \n",
    "    # Separate the DataFrame into two parts\n",
    "    df1 = df[['Order ID', 'Product', 'Quantity Ordered', 'Price Each', 'Order Date', 'Purchase Address', 'Source_File']]\n",
    "    df2 = df[['Order_ID', 'Product', 'Quantity_Ordered', 'Price_Each', 'Order_Date', 'Purchase_Address', 'Source_File']]\n",
    "    \n",
    "    # Rename columns in df1\n",
    "    df1 = df1.rename(columns=column_mappings)\n",
    "    \n",
    "    # Function to convert to numeric, handling errors\n",
    "    def safe_numeric(x):\n",
    "        return pd.to_numeric(x, errors='coerce')\n",
    "    \n",
    "    # Correct data types for both dataframes\n",
    "    for df in [df1, df2]:\n",
    "        df['Order_ID'] = safe_numeric(df['Order_ID'])\n",
    "        df['Quantity_Ordered'] = safe_numeric(df['Quantity_Ordered'])\n",
    "        df['Price_Each'] = safe_numeric(df['Price_Each'])\n",
    "        df['Order_Date'] = pd.to_datetime(df['Order_Date'], errors='coerce')\n",
    "    \n",
    "    # Combine the dataframes\n",
    "    df_combined = pd.concat([df1, df2], ignore_index=True)\n",
    "    \n",
    "    # Drop rows with all NaN values\n",
    "    df_combined = df_combined.dropna(how='all')\n",
    "    \n",
    "    # Remove duplicate rows\n",
    "    df_combined = df_combined.drop_duplicates()\n",
    "    \n",
    "    print(\"\\nHarmonized DataFrame structure:\")\n",
    "    print(df_combined.info())\n",
    "    \n",
    "    # Save the harmonized DataFrame\n",
    "    harmonized_file_path = file_path.replace('.csv', '_harmonized.csv')\n",
    "    df_combined.to_csv(harmonized_file_path, index=False)\n",
    "    print(f\"\\nHarmonized DataFrame saved to: {harmonized_file_path}\")\n",
    "    \n",
    "    return df_combined\n",
    "\n",
    "# Specify the path to your combined CSV file\n",
    "file_path = r\"C:\\Users\\DELL\\OneDrive\\Capstone_Project\\data\\combined_sales_data_2019.csv\"\n",
    "\n",
    "# Harmonize and merge the DataFrame\n",
    "df_final = harmonize_and_merge_dataframe(file_path)\n",
    "\n",
    "# Now you can work with df_final, which should have the correct structure and data types\n",
    "print(\"\\nFinal DataFrame preview:\")\n",
    "print(df_final.head())\n",
    "\n",
    "# Basic data quality checks\n",
    "print(\"\\nData quality checks:\")\n",
    "print(f\"Number of null values:\\n{df_final.isnull().sum()}\")\n",
    "print(f\"\\nNumber of unique values in each column:\\n{df_final.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The file 'C:\\Users\\DELL\\OneDrive\\Capstone_Project\\data\\combined_sales_data_2019_harmonized.csv' was not found.\n",
      "Failed to load the CSV file. Please check the file path and try again.\n"
     ]
    }
   ],
   "source": [
    "def load_harmonized_csv(file_path):\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        combine_data = pd.read_csv(file_path)\n",
    "        \n",
    "        print(f\"Successfully loaded data from {file_path}\")\n",
    "        print(f\"Shape of the DataFrame: {combine_data.shape}\")\n",
    "        \n",
    "        # Display basic information about the DataFrame\n",
    "        print(\"\\nDataFrame Info:\")\n",
    "        print(combine_data.info())\n",
    "        \n",
    "        # Display the first few rows\n",
    "        print(\"\\nFirst few rows of the DataFrame:\")\n",
    "        print(combine_data.head())\n",
    "        \n",
    "        # Check for missing values\n",
    "        print(\"\\nMissing values in each column:\")\n",
    "        print(combine_data.isnull().sum())\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(\"\\nSummary statistics:\")\n",
    "        print(combine_data.describe())\n",
    "        \n",
    "        return combine_data\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_path}' was not found.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Error: The file '{file_path}' is empty.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Error: Unable to parse '{file_path}'. Please ensure it's a valid CSV file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Specify the path to your harmonized CSV file\n",
    "file_path = r\"C:\\Users\\DELL\\OneDrive\\Capstone_Project\\data\\combined_sales_data_2019_harmonized.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame named combine_data\n",
    "combine_data = load_harmonized_csv(file_path)\n",
    "\n",
    "if combine_data is not None:\n",
    "    print(\"\\nDataFrame 'combine_data' is ready for further analysis.\")\n",
    "    \n",
    "    # You can now work with the DataFrame 'combine_data'\n",
    "    # For example, you can perform further analysis or data manipulation here\n",
    "    \n",
    "    # Check the data types of the columns\n",
    "    print(\"\\nData types of the columns:\")\n",
    "    print(combine_data.dtypes)\n",
    "    \n",
    "    # Check for unique values in categorical columns\n",
    "    categorical_columns = ['Product', 'Purchase_Address', 'Source_File']\n",
    "    for col in categorical_columns:\n",
    "        print(f\"\\nUnique values in {col}: {combine_data[col].nunique()}\")\n",
    "        print(f\"Top 5 most common {col}:\")\n",
    "        print(combine_data[col].value_counts().head())\n",
    "else:\n",
    "    print(\"Failed to load the CSV file. Please check the file path and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\DELL\\\\OneDrive\\\\Capstone_Project\\\\data\\\\combined_sales_data_2019_harmonized.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m output_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDELL\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCapstone_Project\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcombined_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Load, rename, and save the DataFrame\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m combined_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_rename_save_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Now you can work with the 'combined_data' DataFrame\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# For example, you can perform further analysis or data manipulation here\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mData types of the columns:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m, in \u001b[0;36mload_rename_save_dataframe\u001b[1;34m(input_file_path, output_file_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_rename_save_dataframe\u001b[39m(input_file_path, output_file_path):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Load the CSV file into a DataFrame\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Rename the DataFrame (this step is conceptual, as DataFrames don't have names in Python)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     combined_data \u001b[38;5;241m=\u001b[39m df\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive\\Capstone_Project\\virt_caps\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive\\Capstone_Project\\virt_caps\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive\\Capstone_Project\\virt_caps\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive\\Capstone_Project\\virt_caps\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive\\Capstone_Project\\virt_caps\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\DELL\\\\OneDrive\\\\Capstone_Project\\\\data\\\\combined_sales_data_2019_harmonized.csv'"
     ]
    }
   ],
   "source": [
    "def load_rename_save_dataframe(input_file_path, output_file_path):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_file_path)\n",
    "    \n",
    "    # Rename the DataFrame (this step is conceptual, as DataFrames don't have names in Python)\n",
    "    combined_data = df\n",
    "    \n",
    "    print(\"DataFrame information:\")\n",
    "    print(combined_data.info())\n",
    "    \n",
    "    # Display the first few rows\n",
    "    print(\"\\nFirst few rows of the DataFrame:\")\n",
    "    print(combined_data.head())\n",
    "    \n",
    "    # Save the DataFrame as a new CSV file\n",
    "    combined_data.to_csv(output_file_path, index=False)\n",
    "    print(f\"\\nDataFrame saved as: {output_file_path}\")\n",
    "    \n",
    "    return combined_data\n",
    "\n",
    "# Specify the paths\n",
    "input_file_path = r\"C:\\Users\\DELL\\OneDrive\\Capstone_Project\\data\\combined_sales_data_2019_harmonized.csv\"\n",
    "output_file_path = r\"C:\\Users\\DELL\\OneDrive\\Capstone_Project\\data\\combined_data.csv\"\n",
    "\n",
    "# Load, rename, and save the DataFrame\n",
    "combined_data = load_rename_save_dataframe(input_file_path, output_file_path)\n",
    "\n",
    "# Now you can work with the 'combined_data' DataFrame\n",
    "# For example, you can perform further analysis or data manipulation here\n",
    "\n",
    "print(\"\\nData types of the columns:\")\n",
    "print(combined_data.dtypes)\n",
    "\n",
    "print(\"\\nBasic statistics of the numerical columns:\")\n",
    "print(combined_data.describe())\n",
    "\n",
    "print(\"\\nNumber of null values in each column:\")\n",
    "print(combined_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 185950 entries, 0 to 185949\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   Order_ID          185686 non-null  float64\n",
      " 1   Product           185938 non-null  object \n",
      " 2   Quantity_Ordered  185686 non-null  float64\n",
      " 3   Price_Each        185686 non-null  float64\n",
      " 4   Order_Date        185686 non-null  object \n",
      " 5   Purchase_Address  185698 non-null  object \n",
      " 6   Source_File       185950 non-null  object \n",
      "dtypes: float64(3), object(4)\n",
      "memory usage: 9.9+ MB\n"
     ]
    }
   ],
   "source": [
    "combined_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order_ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity_Ordered</th>\n",
       "      <th>Price_Each</th>\n",
       "      <th>Order_Date</th>\n",
       "      <th>Purchase_Address</th>\n",
       "      <th>Source_File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>176558.0</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.95</td>\n",
       "      <td>2019-04-19 08:46:00</td>\n",
       "      <td>917 1st St, Dallas, TX 75001</td>\n",
       "      <td>Sales_April_2019.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sales_April_2019.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176559.0</td>\n",
       "      <td>Bose SoundSport Headphones</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.99</td>\n",
       "      <td>2019-04-07 22:30:00</td>\n",
       "      <td>682 Chestnut St, Boston, MA 02215</td>\n",
       "      <td>Sales_April_2019.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>176560.0</td>\n",
       "      <td>Google Phone</td>\n",
       "      <td>1.0</td>\n",
       "      <td>600.00</td>\n",
       "      <td>2019-04-12 14:38:00</td>\n",
       "      <td>669 Spruce St, Los Angeles, CA 90001</td>\n",
       "      <td>Sales_April_2019.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>176560.0</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.99</td>\n",
       "      <td>2019-04-12 14:38:00</td>\n",
       "      <td>669 Spruce St, Los Angeles, CA 90001</td>\n",
       "      <td>Sales_April_2019.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Order_ID                     Product  Quantity_Ordered  Price_Each  \\\n",
       "0  176558.0        USB-C Charging Cable               2.0       11.95   \n",
       "1       NaN                         NaN               NaN         NaN   \n",
       "2  176559.0  Bose SoundSport Headphones               1.0       99.99   \n",
       "3  176560.0                Google Phone               1.0      600.00   \n",
       "4  176560.0            Wired Headphones               1.0       11.99   \n",
       "\n",
       "            Order_Date                      Purchase_Address  \\\n",
       "0  2019-04-19 08:46:00          917 1st St, Dallas, TX 75001   \n",
       "1                  NaN                                   NaN   \n",
       "2  2019-04-07 22:30:00     682 Chestnut St, Boston, MA 02215   \n",
       "3  2019-04-12 14:38:00  669 Spruce St, Los Angeles, CA 90001   \n",
       "4  2019-04-12 14:38:00  669 Spruce St, Los Angeles, CA 90001   \n",
       "\n",
       "            Source_File  \n",
       "0  Sales_April_2019.csv  \n",
       "1  Sales_April_2019.csv  \n",
       "2  Sales_April_2019.csv  \n",
       "3  Sales_April_2019.csv  \n",
       "4  Sales_April_2019.csv  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virt_caps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
